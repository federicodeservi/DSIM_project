{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from IPython import display\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"rm\" non Š riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n",
      "Sottodirectory o file data gi… esistente.\n",
      "Sintassi del comando errata.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!mkdir data/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "file_id = '1jwIWW2vuBJVO-XcCTL9HgmcolCfQJ2ir'    \n",
    "download_file_from_google_drive(file_id, \"data/data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zf = ZipFile('data/data.zip', 'r')\n",
    "zf.extractall('data/dataset')\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"data/dataset/data_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform(file_path):\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, audio_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length = 255\n",
    "frame_step = 255\n",
    "num_mel_bins = 75\n",
    "num_spectrogram_bins = (frame_length // 2) + 1\n",
    "fmin = 0.0\n",
    "sample_rate = 44100\n",
    "fmax = sample_rate / 2\n",
    "\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "    # Padding for files with less than 16000 samples\n",
    "    zero_padding = tf.zeros([140000] - tf.shape(waveform), dtype=tf.float32)\n",
    "    # Concatenate audio with padding so that all audio clips will be of the \n",
    "    # same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    magnitude_spectrograms  = tf.signal.stft(\n",
    "      equal_length, frame_length, frame_step)\n",
    "    magnitude_spectrograms  = tf.abs(magnitude_spectrograms)\n",
    "    \n",
    "    # Step: magnitude_spectrograms->mel_spectrograms\n",
    "    # Warp the linear-scale, magnitude spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = magnitude_spectrograms.shape[-1]\n",
    "\n",
    "\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sample_rate, fmin,\n",
    "        fmax)\n",
    "\n",
    "    mel_spectrograms = tf.tensordot(\n",
    "        magnitude_spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "\n",
    "    mel_spectrograms.set_shape(magnitude_spectrograms.shape[:-1].concatenate(\n",
    "  linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    #mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n",
    "    #  log_mel_spectrograms)[..., :75]\n",
    "  \n",
    "    return log_mel_spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, label, class_path, i):\n",
    "  fig, ax = plt.subplots(figsize=(20,20))\n",
    "  mfcc_data= np.swapaxes(spectrogram, 0 ,1)\n",
    "  cax = ax.imshow(mfcc_data, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "  ax.axis(\"off\")\n",
    "  fig.savefig('data\\dataset\\mels{}\\mel_{}_{}.png'.format(class_path.split(\"data\\dataset\\data_final\")[1], label, i), bbox_inches='tight', pad_inches=0, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    #TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    #Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    augmented_samples = augment(samples=y, sample_rate=sr)\n",
    "    return augmented_samples, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\dataset\\data_final\\Alces_alces\n",
      "data\\dataset\\data_final\\Alces_alces\\01.wav\n",
      "data/dataset/data_final/Alces_alces/300.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\02.wav\n",
      "data/dataset/data_final/Alces_alces/301.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\03.wav\n",
      "data/dataset/data_final/Alces_alces/302.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\04.wav\n",
      "data/dataset/data_final/Alces_alces/303.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\05.wav\n",
      "data/dataset/data_final/Alces_alces/304.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\06.wav\n",
      "data/dataset/data_final/Alces_alces/305.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\07.wav\n",
      "data/dataset/data_final/Alces_alces/306.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\08.wav\n",
      "data/dataset/data_final/Alces_alces/307.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\09.wav\n",
      "data/dataset/data_final/Alces_alces/308.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\10.wav\n",
      "data/dataset/data_final/Alces_alces/309.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\11.wav\n",
      "data/dataset/data_final/Alces_alces/310.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\12.wav\n",
      "data/dataset/data_final/Alces_alces/311.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\13.wav\n",
      "data/dataset/data_final/Alces_alces/312.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\14.wav\n",
      "data/dataset/data_final/Alces_alces/313.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\15.wav\n",
      "data/dataset/data_final/Alces_alces/314.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\16.wav\n",
      "data/dataset/data_final/Alces_alces/315.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\17.wav\n",
      "data/dataset/data_final/Alces_alces/316.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\18.wav\n",
      "data/dataset/data_final/Alces_alces/317.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\19.wav\n",
      "data/dataset/data_final/Alces_alces/318.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\20.wav\n",
      "data/dataset/data_final/Alces_alces/319.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\21.wav\n",
      "data/dataset/data_final/Alces_alces/320.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\22.wav\n",
      "data/dataset/data_final/Alces_alces/321.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\23.wav\n",
      "data/dataset/data_final/Alces_alces/322.wav\n",
      "data\\dataset\\data_final\\Alces_alces\\24.wav\n",
      "data/dataset/data_final/Alces_alces/323.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\n",
      "data\\dataset\\data_final\\Bos_taurus\\00.wav\n",
      "data/dataset/data_final/Bos_taurus/300.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\01.wav\n",
      "data/dataset/data_final/Bos_taurus/301.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\02.wav\n",
      "data/dataset/data_final/Bos_taurus/302.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\03.wav\n",
      "data/dataset/data_final/Bos_taurus/303.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\04.wav\n",
      "data/dataset/data_final/Bos_taurus/304.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\05.wav\n",
      "data/dataset/data_final/Bos_taurus/305.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\06.wav\n",
      "data/dataset/data_final/Bos_taurus/306.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\07.wav\n",
      "data/dataset/data_final/Bos_taurus/307.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\08.wav\n",
      "data/dataset/data_final/Bos_taurus/308.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\09.wav\n",
      "data/dataset/data_final/Bos_taurus/309.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\10.wav\n",
      "data/dataset/data_final/Bos_taurus/310.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\11.wav\n",
      "data/dataset/data_final/Bos_taurus/311.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\12.wav\n",
      "data/dataset/data_final/Bos_taurus/312.wav\n",
      "data\\dataset\\data_final\\Bos_taurus\\13.wav\n"
     ]
    }
   ],
   "source": [
    "main_dir = \"data/dataset/data_final\"\n",
    "folders_pathlist = Path(main_dir).glob('*')\n",
    "\n",
    "for path in folders_pathlist:\n",
    "    # because path is object not string\n",
    "    class_path = str(path)\n",
    "    print(class_path)\n",
    "    wav_pathlist = Path(class_path).glob('./' + ('[0-9]' * 2) + \".wav\")\n",
    "    i = 300\n",
    "    for w_path in wav_pathlist:\n",
    "        print(w_path)\n",
    "        wav_path = str(w_path)\n",
    "        label = get_label(wav_path)\n",
    "        augmented_samples, sr = augmentation(wav_path)\n",
    "        print('data/dataset/data_final/{}/{}.wav'.format(label,i))\n",
    "        sf.write('data/dataset/data_final/{}/{}.wav'.format(label,i), augmented_samples, sr)\n",
    "        i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    os.makedirs('data/dataset/mels')\n",
    "except:\n",
    "    print(\"Folder already exists, deleting its content to replace them with new ones.\")\n",
    "    shutil.rmtree('data/dataset/mels')\n",
    "    os.makedirs('data/dataset/mels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"data/dataset/data_final\"\n",
    "folders_pathlist = Path(main_dir).glob('*')\n",
    "\n",
    "for path in folders_pathlist:\n",
    "    # because path is object not string\n",
    "    class_path = str(path)\n",
    "    print(class_path)\n",
    "    wav_pathlist = Path(class_path).glob('*')\n",
    "    i = 0\n",
    "    os.makedirs('data/dataset/mels/{}'.format(class_path.rsplit(\"data_final\\\\\")[1]))\n",
    "    for w_path in wav_pathlist:\n",
    "        wav_path = str(w_path)\n",
    "        wave, _ = get_waveform(wav_path)\n",
    "        label = get_label(wav_path)\n",
    "        mel = get_spectrogram(wave)\n",
    "        plot_spectrogram(mel, label, class_path, i)\n",
    "        i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide dataset in train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio(\"data/dataset/mels\", output=\"data/dataset/mel_final\", seed=1337, ratio=(.8, .1, .1)) # default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
