{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMshJv8OHVmu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23hFexfVw5Cl"
   },
   "outputs": [],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!mkdir data/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr1wgp8a6Kmx"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "file_id = '1jwIWW2vuBJVO-XcCTL9HgmcolCfQJ2ir'    \n",
    "download_file_from_google_drive(file_id, \"data/data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uy52C36z7Qxj",
    "outputId": "d224aad2-bcac-49b1-cc34-0670b271b29d"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "zf = ZipFile('data/data.zip', 'r')\n",
    "zf.extractall('data/dataset')\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzLKpmZICaWN",
    "outputId": "9da4ac00-3c72-426f-916c-00e8d970d7f9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from IPython import display\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# Set seed \n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#check if gpu is used\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-rayb7-3Y0I"
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"data/dataset/data_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgvFq3uYiS5G"
   },
   "source": [
    "Check basic statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70IBxSKxA1N9",
    "outputId": "001143d4-fcbb-4ff0-de3f-3d57545eca08"
   },
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "n_classes= len(commands)\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMvdU9SY8WXN"
   },
   "source": [
    "Extract the audio files into a list and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlX685l1wD9k",
    "outputId": "f35d9be7-8deb-43f8-f1f8-8b110df82b06"
   },
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
    "print('Example file tensor:', filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vK3ymy23MCP"
   },
   "source": [
    "Split the files into training, validation and test sets using a 80:10:10 ratio, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cv_wts-l3KgD",
    "outputId": "8b73d378-d2bc-44b1-9e32-2d97e80c272e"
   },
   "outputs": [],
   "source": [
    "train_files = filenames[:int(num_samples*0.8)]\n",
    "val_files = filenames[int(num_samples*0.8): int(num_samples*0.9)]\n",
    "test_files = filenames[-int(num_samples*0.1):]\n",
    "\n",
    "print('Training set size', (train_files.shape[0]))\n",
    "print('Validation set size', (val_files.shape[0]))\n",
    "print('Test set size', (test_files.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2Cj9FyvfweD"
   },
   "source": [
    "## Reading audio files and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PjJ2iXYwftD"
   },
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPQseZElOjVN"
   },
   "source": [
    "The label for each WAV file is its parent directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VTtX1nr3YT-"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8Y9w_5MOsr-"
   },
   "source": [
    "Let's define a method that will take in the filename of the WAV file and output a tuple containing the audio and labels for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdgUD5T93NyT"
   },
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvN8W_dDjYjc"
   },
   "source": [
    "You will now apply `process_path` to build your training set to extract the audio-label pairs and check the results. You'll build the validation and test sets using a similar procedure later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SQl8yXl3kNP"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0aT8NgUPdru",
    "outputId": "e306c827-b401-43e9-8655-3617ced086cb"
   },
   "outputs": [],
   "source": [
    "waveform_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxGEwvuh2L7"
   },
   "source": [
    "Let's examine a few audio waveforms with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "8yuX6Nqzf6wT",
    "outputId": "ff01d6f0-22b3-4f49-87ae-9c6860f16b0d"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(audio.numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  ax.set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWXPphxm0B4m"
   },
   "source": [
    "## Spectrogram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFI7bPS_njms"
   },
   "outputs": [],
   "source": [
    "frame_length = 2048\n",
    "frame_step = 512\n",
    "num_mel_bins = 128\n",
    "num_spectrogram_bins = (frame_length // 2) + 1\n",
    "fmin = 0.0\n",
    "sample_rate = 44100\n",
    "fmax = sample_rate / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVEoE3C6Q97_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4CK75DHz_OR"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Padding for files with less than 16000 samples\n",
    "    zero_padding = tf.zeros([140000] - tf.shape(waveform), dtype=tf.float32)\n",
    "    # Concatenate audio with padding so that all audio clips will be of the \n",
    "    # same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    magnitude_spectrograms  = tf.signal.stft(\n",
    "      equal_length, frame_length, frame_step)\n",
    "    magnitude_spectrograms  = tf.abs(magnitude_spectrograms)\n",
    "    \n",
    "    # Step: magnitude_spectrograms->mel_spectrograms\n",
    "    # Warp the linear-scale, magnitude spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = magnitude_spectrograms.shape[-1]\n",
    "\n",
    "\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sample_rate, fmin,\n",
    "        fmax)\n",
    "\n",
    "    mel_spectrograms = tf.tensordot(\n",
    "        magnitude_spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "\n",
    "    mel_spectrograms.set_shape(magnitude_spectrograms.shape[:-1].concatenate(\n",
    "  linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    #mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n",
    "    #  log_mel_spectrograms)[..., :75]\n",
    "  \n",
    "    return log_mel_spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rdPiPYJphs2"
   },
   "source": [
    "Next, you will explore the data. Compare the waveform, the spectrogram and the actual audio of one example from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "4Mu6Y7Yz3C-V",
    "outputId": "049b7bfe-21d5-49fd-8e7f-8dafbd4ddb40"
   },
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(147):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "e62jzb36-Jog",
    "outputId": "ebc5b1af-14fc-4f96-efd7-cd651038c5d0"
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram):\n",
    "  fig, ax = plt.subplots(figsize=(20,20))\n",
    "  mfcc_data= np.swapaxes(spectrogram, 0 ,1)\n",
    "  cax = ax.imshow(mfcc_data, interpolation='nearest', cmap=cm.coolwarm, origin='lower')\n",
    "  ax.set_title('Spectrogram')\n",
    "\n",
    "plot_spectrogram(spectrogram)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyYXjW07jCHA"
   },
   "source": [
    "Now transform the waveform dataset to have spectrogram images and their corresponding labels as integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43IS2IouEV40"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "  spectrogram = tf.image.grayscale_to_rgb(spectrogram)\n",
    "  label_id = tf.argmax(label == commands)\n",
    "  return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEVb_oK0oBLQ"
   },
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5KdY8IF8rkt"
   },
   "source": [
    "## Build and train the model\n",
    "\n",
    "Now you can build and train your model. But before you do that, you'll need to repeat the training set preprocessing on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10UI32QH_45b"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      get_spectrogram_and_label_id)\n",
    "  return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNv4xwYkB2P6"
   },
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assnWo6SB3lR"
   },
   "source": [
    "Batch the training and validation sets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgY9WYzn61EX"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS1uIh6F_TN9"
   },
   "source": [
    "Add dataset [`cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) and [`prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) operations to reduce read latency while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdZ6M-F5_QzY"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwHkKCQQb5oW"
   },
   "source": [
    "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n",
    "The model also has the following additional preprocessing layers:\n",
    "- A [`Resizing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing) layer to downsample the input to enable the model to train faster.\n",
    "- A [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization) layer to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (i.e. mean and standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALYz7PFCHblP",
    "outputId": "12a708be-647b-454b-88df-85c3cf4d53d4"
   },
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3LR3LJ71RL8"
   },
   "outputs": [],
   "source": [
    "#base_net = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False,\n",
    "#\tinput_shape=input_shape, pooling=\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcaCBsKn1QzR"
   },
   "outputs": [],
   "source": [
    "#for layer in base_net.layers:\n",
    "#    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aa1fIgA1Qmn"
   },
   "outputs": [],
   "source": [
    "#Output of the base_net model\n",
    "#x = base_net.output\n",
    "# intermediate fully-connected layer + ReLU\n",
    "#x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "# final fully-connected layer + SoftMax \n",
    "#pred = tf.keras.layers.Dense(n_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKZ8bf2L1QUj"
   },
   "outputs": [],
   "source": [
    "#model = tf.keras.Model(inputs=base_net.input, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6fqXFwq9LOK",
    "outputId": "3bf31988-fccd-40ca-e112-ce1910dd8bb7"
   },
   "outputs": [],
   "source": [
    "model1 = models.Sequential(\n",
    "    [\n",
    "        layers.Input(shape = input_shape),\n",
    "        preprocessing.Resizing(32, 32), \n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, kernel_size=(3,3), activation = 'relu'),\n",
    "        layers.Conv2D(32, kernel_size=(3,3), activation = 'relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Conv2D(64, kernel_size=(3,3), activation = 'relu'),\n",
    "        layers.Conv2D(64, kernel_size=(3,3), activation = 'relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(commands), activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFjj7-EmsTD-"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"model_weights4.hdf5\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttioPJVMcGtq",
    "outputId": "a238d612-77c9-4791-b688-1ae7ba508759",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS, callbacks=[model_checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjpCDeQ4mUfS"
   },
   "source": [
    "Let's check the training and validation loss curves to see how your model has improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "nzhipg3Gu2AY",
    "outputId": "1a083c44-aa58-4263-d46b-4b64229c0097"
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4"
   },
   "source": [
    "## Evaluate test set performance\n",
    "\n",
    "Let's run the model on the test set and check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biU2MwzyAo8o"
   },
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktUanr9mRZky",
    "outputId": "573a1836-e35e-40b0-d528-7675f8ed2b57"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH"
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "A confusion matrix is helpful to see how well the model did on each of the commands in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "LvoSAOiXU3lL",
    "outputId": "2ee5fcd9-d270-479a-c07b-5468954fe75d"
   },
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQGi_mzPcLvl"
   },
   "source": [
    "## Run inference on an audio file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "zRxauKMdhofU",
    "outputId": "7411db4a-6731-4b5b-914d-cec16d012451"
   },
   "outputs": [],
   "source": [
    "sample_file = test_files[2].numpy().decode(\"UTF-8\")\n",
    "\n",
    "sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "  prediction = model(spectrogram)\n",
    "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7wgjg48Vom7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "animal_species_audio.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
